# -*- coding: utf-8 -*-
"""Traffic Sign Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j7gYFuSE8dxhYEDt0j8vt5FXuCKLXy8d
"""

from google.colab import drive
drive.mount("/content/gdrive")

import datetime
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPool2D

import tensorflow as tf
print("TF version: ", tf.__version__)

device_name = tf.test.gpu_device_name()
if "GPU" not in device_name:
    print("No")
else:
    print(device_name)

train_df = pd.read_csv('/content/gdrive/MyDrive/dataset/Train.csv')
train_df.describe()

train_df = train_df.drop(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2'], axis = 1)
train_df.head()

train_df['ClassId'].value_counts().plot.bar(figsize=(20, 10))
train_df['ClassId'].value_counts().median()

filenames = ['/content/gdrive/MyDrive/dataset/' + fname for fname in train_df['Path']]
filenames[:10]

labels = train_df['ClassId'].to_numpy()
labels

unique_signs = np.unique(labels)
len(unique_signs)

labels = tf.keras.utils.to_categorical(labels, 43)
labels[0]

len(labels)

X = filenames
y = labels


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)
len(X_train), len(y_train), len(X_val), len(y_val)

IMG_SIZE = 32

def process_image(image_path):
    """
    Takes an image file path and turns the image into a Tensor.
    """
    # Read in an image file
    image = tf.io.read_file(image_path)
    # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)
    image = tf.image.decode_png(image, channels=3)
    # Convert the colour channel values from 0-255 to 0-1 values
    image = tf.image.convert_image_dtype(image, tf.float32)
    # Resize the image to our desired value (32, 32)
    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
    return image

def get_image_label (image_path, label):
    """
    Takes an image file path name and the assosciated label,
    processes the image and reutrns a typle of (image, label).
    """
    image = process_image(image_path)
    return image, label

BATCH_SIZE = 64

# Create a function to turn data into batches
def create_data_batches (X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
    """
    Creates batches of data out of image (X) and label (y) pairs.
    Shuffles the data if it's training data but doesn't shuffle if it's validation dat
    a.
    Also accepts test data as input (no labels).
    """
    # If the data is a test dataset, we probably don't have have labels
    if test_data:
        print("Creating test data batches...")
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))
        data_batch = data.map(process_image).batch(BATCH_SIZE)
    # If the data is a valid dataset, we don't need to shuffle it
    elif valid_data:
        print("Creating validation dataset batches...")
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))
        # Create (image, label) tuples (this also turns the iamge path into a preprocessed image)
        data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    else:
        print("Creating training dataset batches...")
        # Turn filepaths and labels into Tensors
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))
        # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images
        data = data.shuffle(buffer_size=len(X))
        # Create (image, label) tuples (this also turns the iamge path into a preprocessed image) and turning into batches
        data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

# Creating training and validation batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attributes of our data batches
train_data.element_spec, val_data.element_spec

def show_25_images (images, labels):
    """
    Displays a plot of 25 images and their labels from a data batch.
    """
    plt.figure(figsize=(10,10))
    for i in range(25):
        ax = plt.subplot(5, 5, i+1)
        plt.imshow(images[i])
        plt.title(unique_signs[labels[i].argmax()])
        plt.axis("off")

train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

# Visualizing validation data
val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

# Setup input shape to the model
INPUT_SHAPE = [IMG_SIZE, IMG_SIZE, 3]

# Setup the output shape
OUTPUT_SHAPE = len(unique_signs)

def traffic_sign_net(input_shape):
    model = Sequential()
    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))
    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(Dropout(rate=0.25))
    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(Dropout(rate=0.25))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(43, activation='softmax'))
    return model

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE):
    # Setup the model layers
    model = traffic_sign_net(input_shape=input_shape)
    # Compile the model
    print("Compiling the model")
    model.compile(
        loss = tf.keras.losses.CategoricalCrossentropy(),
        optimizer = tf.keras.optimizers.Adam(),
        metrics = ["accuracy"]
    )
    return model

model = create_model()
model.summary()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)

NUM_EPOCHS = 15

# Build a fn to train and return a trained model
def train_model():
    """
    Trains a given model and returns the trained version.
    """
    # Create a model
    model = create_model()
    # Fit the model to the data passing it the callbacks we created
    model.fit(x=train_data,
        epochs=NUM_EPOCHS,
        validation_data=val_data,
        validation_freq=1,
        callbacks=[early_stopping]
             )
    return model

# Fit the model to data
model = train_model()

model.save('Model.h5')

evaluation = pd.DataFrame(model.history.history)

evaluation[['accuracy', 'val_accuracy']].plot()
evaluation[['loss', 'val_loss']].plot()

from tensorflow.keras.models import load_model
model = load_model('/content/gdrive/MyDrive/Model.h5')

test_path = '/content/gdrive/MyDrive/dataset/Test'
test_images = sorted(os.listdir(test_path))

from PIL import Image

def scaling(test_images, test_path):
    images = []

    image_path = test_images
    
    for x in image_path:
        img = Image.open(test_path + '/' + x)
        img = img.resize((32,32))
        img = np.array(img)
        images.append(img)

    #Converting images into numpy array
    images = np.array(images)
    #The pixel value of each image ranges between 0 and 255
    #Dividing each image by 255 will scale the values between 0 and 1. This is also known as normalization.
    images = images/255

    return images

test_images = scaling(test_images,test_path)

from PIL import Image

def scaling(test_images, test_path):
    images = []

    image_path = test_images
    
    for x in image_path:
        img = Image.open(test_path + '/' + x)
        img = img.resize((32,32))
        img = np.array(img)
        images.append(img)

    #Converting images into numpy array
    images = np.array(images)
    #The pixel value of each image ranges between 0 and 255
    #Dividing each image by 255 will scale the values between 0 and 1. This is also known as normalization.
    images = images/255

    return images



import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns

from tensorflow.keras.utils import to_categorical

test_images = scaling(test_imgf,test_path)

test = pd.read_csv('/content/gdrive/MyDrive/dataset/Test.csv')

y_test = test['ClassId'].values

y_test

len(y_test)

print(test)

y_pred = model.predict_classes(test_images)

print(y_pred)

len(y_pred)

error=np.where(y_test!=y_pred) #Class Id's differences

for x in error:
    print("Original Id=",y_test[x],"Predicted Id=",y_pred[x])
    
print(error)#Indexces where Id's dont match

from sklearn.metrics import classification_report

print(classification_report(y_pred,y_test))

/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)
    210     if len(uniques) > 1:
    211         raise ValueError("Found input variables with inconsistent numbers of"
--> 212                          " samples: %r" % [int(l) for l in lengths])
    213 
    214 

ValueError: Found input variables with inconsistent numbers of samples: [3724, 12630]

/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)
    210     if len(uniques) > 1:
    211         raise ValueError("Found input variables with inconsistent numbers of"
--> 212                          " samples: %r" % [int(l) for l in lengths])
    213 
    214 

ValueError: Found input variables with inconsistent numbers of samples: [3724, 12630]

cm = confusion_matrix(labels[:-8906], prediction)

print('Test Data Accuracy:', accuracy_score(labels[:-8906], prediction)*100)

from sklearn.metrics import classification_report

print(classification_report(labels[:-8906], prediction))

plt.figure(figsize=(25,25))

start_index = 0

for i in range(25):
    plt.subplot(5,5,i+1)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    prediction = y_pred[start_index + i]
    
    actual = labels[start_index + i]
    
    col = 'g'
    
    if prediction != actual:
        col = 'r'
        
    plt.xlabel('Actual = {} || Pred = {}'.format(actual,prediction), color=col)
    
    plt.imshow(test_images[start_index + i])
    
    
plt.show()

import  seaborn as sns
df_cm = pd.DataFrame(cf, index =  classes, columns= classes)
plt.figure(figsize =(20,20))
sns.heatmap(df_cm, annot=True)

classes = { 0:'Speed limit (20km/h)',
            1:'Speed limit (30km/h)', 
            2:'Speed limit (50km/h)', 
            3:'Speed limit (60km/h)', 
            4:'Speed limit (70km/h)', 
            5:'Speed limit (80km/h)', 
            6:'End of speed limit (80km/h)', 
            7:'Speed limit (100km/h)', 
            8:'Speed limit (120km/h)', 
            9:'No passing', 
            10:'No passing veh over 3.5 tons', 
            11:'Right-of-way at intersection', 
            12:'Priority road', 
            13:'Yield', 
            14:'Stop', 
            15:'No vehicles', 
            16:'Veh > 3.5 tons prohibited', 
            17:'No entry', 
            18:'General caution', 
            19:'Dangerous curve left', 
            20:'Dangerous curve right', 
            21:'Double curve', 
            22:'Bumpy road',
            23:'Slippery road', 
            24:'Road narrows on the right', 
            25:'Road work', 
            26:'Traffic signals', 
            27:'Pedestrians', 
            28:'Children crossing', 
            29:'Bicycles crossing', 
            30:'Beware of ice/snow',
            31:'Wild animals crossing', 
            32:'End speed + passing limits', 
            33:'Turn right ahead', 
            34:'Turn left ahead', 
            35:'Ahead only', 
            36:'Go straight or right', 
            37:'Go straight or left', 
            38:'Keep right', 
            39:'Keep left', 
            40:'Roundabout mandatory', 
            41:'End of no passing', 
            42:'End no passing veh > 3.5 tons' }

from sklearn.metrics import confusion_matrix

cf = confusion_matrix(labels[:-8906], prediction)

pred = model.predict_classes(test_images)

print('Test Data Accuracy:', accuracy_score(labels,pred)*100)

prediction=np.argmax(model.predict(test_images), axis=-1)

prediction

len(prediction)

labels = test["ClassId"].values

len(labels)

print('Test Data Accuracy:', accuracy_score(labels[:-8906], prediction)*100)

labels.shape

x=labels.reshape(-1)

y=y_pred.shape

y=y_pred.reshape(-1)

from sklearn.metrics import confusion_matrix

cf = confusion_matrix(x, y)

from sklearn.metrics import accuracy_score

from PIL import Image

len(x)

len(y)

from sklearn.linear_model import LinearRegression

model.fit(labels[:-8906], prediction)

print(test)

y_pred = model.predict_classes(test_imgf)

print(y_pred)

test = pd.read_csv(data_dir + '/Test.csv')

labels = test["ClassId"].values
imgs = test["Path"].values

data = []

for img in imgs:
    try:
        image = cv2.imread("/content/gdrive/MyDrive/dataset" + '/' + img)
        image_fromarray = Image.fromarray(image,'RGB')
        resize_image = image_fromarray.resize((Img_hieght, Img_width))
        data.append(np.array(resize_image))
        
    except:
        print("Error in " + img)
        
X_test = np.array(data)
X_test = X_test/255

pred = model.predict_classes(X_test)

print('Test Data Accuracy:', accuracy_score(labels,pred)*100)

import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
np.random.seed(42)

data_dir='/content/gdrive/MyDrive/dataset'
train_path='/content/gdrive/MyDrive/dataset/Train'
test_path='/content/gdrive/MyDrive/dataset/Test'

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *

x = np.random.randint(0,2557,94556)
y = np.eye((2557))[np.random.randint(0,2557,94556)]
xr = x.reshape((-1,1))


print("x.shape: {}\nxr.shape:{}\ny.shape: {}".format(x.shape, xr.shape, y.shape))

y_pred = model.predict_classes(test_images)

error=np.where(y_test!=y_pred) #Class Id's differences

for x in error:
    print("Original Id=",y_test[x],"Predicted Id=",y_pred[x])
    
print(error)#Indexces where Id's dont match

test_images = scaling(test_img,test_path)

test = pd.read_csv('/content/gdrive/MyDrive/dataset/Test.csv')

y_test = test['ClassId'].values

y_test

from PIL import Image

# Fit the model to data
model = train_model()

test_df = pd.read_csv('/content/gdrive/MyDrive/dataset/Test.csv')
test_df = test_df.drop(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2'], axis=1)
test_df.head()

test_img_paths = ['/content/gdrive/MyDrive/dataset/' + path for path in test_df['Path']]
test_img_paths[:10]

X_test = create_data_batches(test_img_paths, test_data=True)
y_test = list(test_df['ClassId'])
y_test[:10]

predictions = model.predict(X_test, verbose=1)

loss, accuracy = model.evaluate(y_pred, y_test)

print('test set accuracy: ', accuracy * 100)